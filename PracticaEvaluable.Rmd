---
title: "Práctica Evaluable: Aprendiendo de los datos"
author: "Juan Antonio Villegas Recio"
date: "`r Sys.Date()`"
lang: es
output:
  html_document: 
    
    toc: true
    number_sections: false
    toc_float: 
      collapsed: false
      smooth_scroll: false
  pdf_document: default
---

```{r,include=F}
# Instalamos los paquetes que no tengamos
# install.packages("EnvStats")
# install.packages("psych")
# install.packages("ggplot2")
# install.packages("factoextra")
# install.packages("polycor")
# install.packages("ggcorrplot")
# install.packages("corrr")
# install.packages("MVN")

# Carga de paquetes
library(EnvStats)
library(car)
library(foreign)
library(psych)
library(ggplot2)
library(reshape2)
library(factoextra)
library(polycor)
library(ggcorrplot)
library(corrplot)
library(stats)
library(reshape2)
library(knitr)
library(dplyr)
library(MVN)
```

------------------------------------------------------------------------

```{=tex}
\begin{center}
 \tableofcontents
 \textsc{--}\\
\end{center}
```

------------------------------------------------------------------------

# Información sobre el Conjunto de datos

En un conjunto constituido por 34 estados del mundo se han observado 11 variables cuyos resultados se recogen en el archivo **`DB_3.sav`**. Estas variables se han estandarizado, pues están tomadas con unidades de medida muy diferentes. Estas variables son:

## Variables

-   `Ztlibrop`: Número de libros publicados.
-   `Ztejerci`: Cociente entre el número de individuos en ejército de tierra y población total del estado.
-   `Ztpobact`: Cociente entre población activa y total.
-   `Ztenergi`: Tasa de consumo energético.
-   `Zpservi`: Población del sector servicios.
-   `Zpagricu`: Población del sector agrícola.
-   `Ztmedico`: Tasa de médicos por habitante.
-   `Zespvida`: Esperanza de vida.
-   `Ztminfan`: Tasa de mortalidad infantil.
-   `Zpobdens`: Densidad de población.
-   `Zpoburb`: Porcentaje de población urbana.

Cargamos los datos a partir del fichero **`DB_3.sav`**.

```{r}
datos_enteros<-read.spss("DB_3.sav", to.data.frame = TRUE)
datos_enteros[11,"PAIS"] <- "espana" # Para evitar problemas de codificacion
datos = datos_enteros
```

## Primera visualización

Echamos un primer vistazo a los datos, observando directamente el dataframe en el que vienen, para ver si los datos efectivamente están estandarizados (datos numéricos, continuos, tanto negativos como positivos y entorno al 0), y para comprobar la existencia de *NA*s.

```{r,echo=F}
head(datos)
```

Vemos que, en efecto, los valores están próximos a 0, por lo que efectivamente están normalizados. Sin embargo, al menos en las primeras 5 filas no hemos encontrado ningún valor perdido.

## Tratamiento de los *NA*

Vamos a ver si hay, dónde, y cuántos *NA* hay en los datos

```{r,echo=F}
cbind(apply(is.na(datos),2,sum),apply(is.na(datos),2,sum)/dim(datos)[1])
```

Como vemos, tan sólo hay un valor perdido en la variable `ZTLIBROP`, concretamente, se desconoce ese valor en Marruecos:

```{r}
datos['marrueco','ZTLIBROP']
```

Por tanto, en conclusión todas las variables distintas de `ZTLIBROP` tienen un 0% de valores perdidos, mientras que `ZTLIBROP` tiene un 2.94% de valores perdidos, lo cual supone un sólo valor perdido que se puede importar, por lo que no es necesario ningún análisis de posibles patrones de los valores perdidos.

Como tan sólo tenemos variables numéricas (salvo `PAIS`), podemos importar este único valor perdido utilizando la media de la columna `ZTLIBROP`.

```{r}
not_available<-function(data,na.rm=F){
  data[is.na(data)]<-mean(data,na.rm=T)
  data
}
datos$ZTLIBROP<-not_available(datos$ZTLIBROP)
```

## Análisis descriptivo numérico

En este apartado iremos variable por variable obteniendo los resultados de aplicar diferentes medidas descriptivas, clásicas y resistentes, de centralidad, forma y dispersión.

```{r,echo=F}
#Definimos las medidas resistentes
PMC<-function(x){ return((as.double(quantile(x,0.25))+as.double(quantile(x,0.75)))/2)}

trimedia<-function(x){return((median(x)+PMC(x))/2)}

centrimedia<-function(x){
  indices<-(x>quantile(x,0.25)&x<quantile(x,0.75))
  valores<-x[indices]
  return(sum(valores)/length(valores))
}

RIQ<-function(x){return(quantile(x,0.75)-quantile(x,0.25))}

MEDA<-function(x){return(median(abs(x-median(x))))}

CVc<-function(x){return((quantile(x,0.75)-quantile(x,0.25))/(quantile(x,0.75)+quantile(x,0.25)))}

H1<-function(x){return((quantile(x,0.25)+quantile(x,0.75)-2*median(x))/(2*median(x)))}
H2<-function(x){return(median(x)-(quantile(x,0.1)+quantile(x,0.9))/(2))}
H3<-function(x){return(H2(x)/median(x))}

#Creamos una función que aplique todas estas medidas

descriptivo<-function(x){
  
  temp<-rbind(PMC(x),trimedia(x),centrimedia(x))
  rownames(temp)<-c("PMC","Trimedia","Centrimedia")
  centralidad<-list(clasica=list(media=mean(x)),resistente=temp)
  
  temp<-rbind(RIQ(x),MEDA(x),CVc(x))
  rownames(temp)<-c("Rango Inter-Cuartílico","MEDA","CVc")
  dispersion<-list(clasica=list(desviación_típica=sd(x),Coef_varización=sd(x)/mean(x),rango=range(x)),resistente=temp)
  
  temp<-rbind(H1(x),H2(x),H3(x))
  rownames(temp)<-c("Asimetría de Yule","Asimetría de Kelly","Asimetría de Kelly adimensional")
  forma<-list(clasica=list(skewness=skewness(x),kurtosis=kurtosis(x)),resistente=temp)
  cat(names(x))
  return(list(centralidad=centralidad,dispersion=dispersion,forma=forma))
}
```

### `ZPOBDENS`: Densidad de población

```{r,echo=F}
descriptivo(datos[,2])
hist(col="darkblue",datos[,2],main="Densidad de población")
```

Al igual que ocurrirá con todas las variables, al estar estandarizadas (normalizadas), la media es $0$ y la desviación típica $1$, por lo que podremos comentar poco acerca de la centralidad y de la dispersión de cada variable por separado. Debemos comentar que el coeficiente de variación llama la atención que sea tan grande, pero se debe a que al dividir por la media, que es prácticamente 0, el resultado es muy grande.

Por otra parte, vemos que el rango de valores no es muy simétrico respecto a la media, lo cual ya nos afirma cierta asimetría en la distribución que podemos confirmar si miramos coeficientes de asimetría como `skewness`, que es positivo, por lo que la distribución es asímetrica a la derecha. El valor de Kurtosis es además menor que tres, por lo que no hay apuntamiento.

Si observamos el histograma, confirmamos la alta concentración de valores entorno al $[-1,0]$ y la asimetría comentada.

### `ZTMINFAN`: Mortalidad infantil

```{r,echo=F}
descriptivo(datos[,3])
hist(col="darkblue",datos[,3],main="Mortalidad infantil")
```

En este caso sí vemos un rango intercuartílico más centrado, lo que nos indica menor dispersión y mayor simetría. Si observamos el histograma, la distribución tiene cierta tendencia a la uniformidad, rota por un exceso en el intervalo $[-1,-0.5]$ y un defecto en el $[0.0, 0.5]$, lo que nos indica que en general la tasa de mortalidad es similar en los paises, pero hay más paises que tienen una tasa baja, mientras que son muy pocos (2) los que tienen una tasa media. El coeficiente de asímetria nos indica que hay asimetría a la derecha, lo cual confirmamos también en el histograma.

### `ZESPVIDA`: Esperanza de vida

```{r,echo=F}
descriptivo(datos[,4])
hist(col="darkblue",datos[,4],main="Esperanza de vida")
```

En este caso, mirando el histograma, la tendencia es clara a una gran asimetría a la izquierda, confirmada por un coeficiente de `skewness` negativo. Por otra parte, el rango, que está considerablemente desviado en torno al 0, nos afirma dispersión, y efectivamente, vemos que hay países con todo tipo de esperanzas de vida, aunque la tendencia es que haya pocos países con esperanzas de vida bajas, mientras que la mayoría tienen esperanzas de vida altas.

### `ZPOBURB`: Porcentaje de población urbana

```{r,echo=F}
descriptivo(datos[,5])
hist(col="darkblue",datos[,5],main="Porcentaje de población urbana")
```

En este caso, sin embargo, los datos están centrados y poco dispersos, como indican el rango (el cual está casi centrado en el $0$) y el rango intercuartílico (el cual es relativamente pequeño), aunque el intervalo modal es el $[0.5,1]$, que no está tan centrado. En general los valores correspondientes a los países están repartidos por todos los intervalos, aunque en los extremos encontramos menos valores, como también es normal, aunque no pueden considerarse realmente outliers ni datos anómalos, pues hay países con muchas ciudades muy pobladas y países muy rurales.

Por otro lado, hay poco apuntamiento, como indica el coeficiente de kurtosis, que es negativo, y poca asimetría, pues el coeficiente `skewness`, aunque es negativo, indicando cierta asimetría a la izquierda, es muy pequeño.

### `ZTMEDICO`: Tasa de médicos por habitante

```{r,echo=F}
descriptivo(datos[,6])
hist(col="darkblue",datos[,6],main="Tasa de médicos por habitante")
```

En una primera observación, vemos incluso cierto parecido con el histograma de la mortalidad infantil. En este caso los valores parecen estar centrados si no contamos los dos países con mayor tasa, que podrían considerarse outliers. Sin embargo, vemos que en general la tendencia es a tener una baja tasa de médicos por habitante. La simetría a la derecha en este caso es clara, aunque no demasiado pronunciada, reflejada por un coeficiente de `skewness` cercano a $0.5$.

### `ZPAGRICU`: Población del sector agrícola

```{r,echo=F}
descriptivo(datos[,7])
hist(col="darkblue",datos[,7],main="Población del sector agrícola")
```

Aquí vemos un rango bastante grande, junto con un rango intercuartílico igual no tan grande, lo cual nos dice que hay datos dispersos aunque la mayoría se concentran en un intervalo pequeño. El coeficiente `skewness` en este caso es bastante positivo, indicando asimetría a la derecha, que podemos confirmar con el histograma.

### `ZPSERVI`: Población del sector servicios

```{r,echo=F}
descriptivo(datos[,8])
hist(col="darkblue",datos[,8],main="Población del sector servicios")
```

En este caso, y al contrario que en la población agrícola, que tendía a ser baja, vemos que la tendencia es a tener una alta población dedicada al sector servicios, probablemente también relacionada con el porcentaje de población urbana, generalmente dedicada a los servicios, más que a la agricultura. Vemos que el rango en este caso, aunque centrado en cero y con un rango intercuartílico pequeño, es poco significativo, pues la mayoría de países concentra su población dedicada al sector servicios en el intervalo $[0,1.5]$, aunque el intervalo de amplitud $0.5$ en el que se sitúan más países es el $[-1,-0.5]$.

Sobre la simetría, la forma es bastante irregular, por lo que las medidas realmente no es que sean demasiado significativas, aunque sí hay una leve asímetría a la izquierda, respaldada con un coeficiente `skewness` pequeño pero negativo.

### `ZTLIBROP`: Número de libros publicados

```{r,echo=F}
descriptivo(datos[,9])
hist(col="darkblue",datos[,9],main="Número de libros publicados")
```

En este caso vemos un rango muy amplio y desviado respecto al cero, luego hay países con números de libros muy dispersos, con muchos y con muy pocos. El rango intercuartílico sin embargo no es muy grande, lo cual unido a un coeficiente `skewness` alto y positivo nos afirma que la mayoría de los países publican pocos libros.

### `ZTEJERCI`: Cociente entre el número de individuos en ejército de tierra y población total del estado

```{r,echo=F}
descriptivo(datos[,10])
hist(col="darkblue",datos[,10],main="Cociente entre el número de individuos en ejército de tierra y población total del estado")
```

La distribución es de hecho muy similar a la de los libros publicados, con un rango incluso más amplio provocado por un outlier que tiene una gran cantidad de personal en el ejército. Sin embargo, vemos un rango intercuartílico muy pequeño, una asimetría alta a la derecha y un kurtosis muy alto también, lo cual nos dice que hay una alta concentración en intervalos pequeños. Esto está respaldado por el histograma, en el que vemos una alta concentración en el intervalo $[-1,1]$ que contrasta con el resto. En conclusión, en los países la tendencia es tener un cociente estándar de población militar respecto a total, pero hay un país, que de hecho podemos confirmar que es 'Israel', que tiene un cociente muy alto en comparación.

### `ZTPOBACT`: Cociente entre población activa y total

```{r,echo=F}
descriptivo(datos[,11])
hist(col="darkblue",datos[,11],main="Cociente entre población activa y total")
```

En este caso tenemos un rango intercuartílico bastante estándar, similar al de casos anteriores, pero un rango muy amplio. Podemos ver en el histograma que hay dos países con un cociente muy bajo, es decir, con poca población activa, que hacen que los datos no estén más centrados.

La distribución se asemeja en forma a una normal más que otras, de hecho el coeficiente `skewness` es muy cercano a cero, sin embargo, hay varias irregularidades notables, las cuales provocan además un kurtosis negativo, es decir, la distribución está aplanada.

### `ZTENERGI`: Tasa de consumo energético

```{r,echo=F}
descriptivo(datos[,12])
hist(col="darkblue",datos[,12],main="Tasa de consumo energético")
```

En una primera observación del histograma, se observa una clara acumulación de valores en torno al intervalo $[-1,-0.5]$, mientras que en los demás intervalos de longitud $0.5$ hay 5 o menos países. De hecho vemos un rango intercuartílico menor que en la mayoría de las variables, menor que $1.40$, mientras que el rango es muy amplio. La forma del histograma y las medidas recuerdan a la de `ZTLIBROP` y a la de `ZTEJERCI`, es decir, una alta concentración en valores bajos y menor en valores altos, presentando incluso outliers.

```{r, echo=F}
summary(datos[,-1])
```

En conclusión, tenemos algunas variables muy asimétricas a la derecha, a la izquierda, algunas que parecen querer aproximarse a una uniforme y otras que parecen querer aproximarse a una normal. Sin embargo, el conjunto de datos es pequeño, tan sólo son 36 ejemplos de países, que no son demasiados. Probablemente con una muestra más representativa podríamos ver tendencias más claras. Sin embargo, con estas muestras hemos visto algunas similitudes entre distribuciones.

## Valores extremos {#valores-extremos}

Primero de todo vamos a observar (de nuevo) la forma de las distribuciones de las variables mediante sus histogramas en un mismo gráfico:

```{r, echo=F}
par(mar=c(1,1,1,1))
par(mfrow=c(3,4))
for (k in 2:12) {
  j0 <- names(datos)[k]
  x <- datos[,k]
  hist(x, main = j0, col = 'darkblue', xlab = NULL, ylab = NULL)
  }
```

Observando los gráficos, de primeras y tal y como veníamos adelantando, no parece que las distribuciones de las variables se parezcan en general a una normal, pues presentan asimetrías. Algunas  de ellas son muy pronunciadas como la de `ZTLIBROP` (fila 2, columna 4) y otras no tanto como las de `ZPAGRICU` (fila 2, columna 2). Aún así, en futuras fases habrá que comprobar y en caso negativo asumir normalidad en las distribuciones para poder aplicar análisis multivariantes.

A continuación y tras un exhaustivo análisis numérico de las variables, con la gran ayuda de los histogramas, procedemos a tratar los valores extremos u *outliers*. Para ello, haremos uso de un gráfico '*boxplot*' que nos ayudará a localizar mejor los valores extremos.

```{r,echo=F}
colfunc<-colorRampPalette(c("darkblue","yellow"))
boxplot(datos[,-1],
        xlab=NULL,
        ylab=NULL,
        col=colfunc(11),
        las=2)
```

En nuestro caso, son `POBDENS`, `ZTEJERCI` y `ZTENERGI`las variables que presentan outliers. Eliminaremos dichos datos importándolos por la media.

```{r,echo=F}
outlier<-function(data,na.rm=T){
  H<-1.5*IQR(data)
  
  if(any(data<=(quantile(data,0.25,na.rm = T)-H))){
    data[data<=quantile(data,0.25,na.rm = T)-H]<-NA
    data[is.na(data)]<-mean(data,na.rm=T)
    data<-outlier(data)}
  
  if(any(data>=(quantile(data,0.75, na.rm = T)+H))){
    data[data>=quantile(data,0.75, na.rm = T)+H]<-NA
    data[is.na(data)]<-mean(data,na.rm=T)
    data<-outlier(data)
  }
  return(data)
}

datos[,c('ZPOBDENS','ZTEJERCI','ZTENERGI')]<-apply(datos[,c('ZPOBDENS','ZTEJERCI','ZTENERGI')], 2, outlier)
# Reescalamos, pues hemos cambiado la distribución 
datos[,c('ZPOBDENS','ZTEJERCI','ZTENERGI')]<-scale(datos[,c('ZPOBDENS','ZTEJERCI','ZTENERGI')]) 
```

Una vez tratados los outliers, vemos de nuevo los gráficos boxplot.

```{r,echo=F}
boxplot(datos[,-1],
        xlab=NULL,
        ylab=NULL,
        col=colfunc(11),
        las=2)
```

Y vemos que efectivamente hemos eliminado los outliers comentados anteriormente. Nuestro conjunto de datos está por tanto en este momento libre de valores perdidos y de outliers.

## Normalidad {#normalidad}

Seguidamente, y debido a que muchos análisis multivariantes que utilizaremos en el futuro requieren hipótesis de normalidad, utilizaremos un gráfico '*qqplot*' para comprobar de una manera más formal, pero aún visual, qué variables se acercan a la normalidad.

```{r,echo=F}
par(mar=c(1,1,1,1))
par(mfrow=c(3,4))
for (k in 2:12) {
  j0 <- names(datos)[k]
  x0 <- seq(min(datos[, k]), max(datos[, k]), le = 50)
  x <- datos[,k]
  qqnorm(x, main = j0)
  qqline(x)
  }
```

Observamos que la mayoría de las variables tienden a acercarse a la diagonal, por ejemplo, `ZPOBURB` (fila 1, columna 4) y `ZTPOBACT` (fila 3, columna 2), ya comentadas anteriormente, se aproximan mucho a la diagonal. Sin embargo, `ZPOBDENS` (fila 1, columna 1) y `ZTMINFAN` (fila 1, columna 2) se alejan más de la diagonal, y por tanto de la distribución normal.

En conclusión, aunque ninguna variable se acerque demasiado, la mayoría se aproximan lo suficiente a la normal, aunque algunas disten más. Este hecho puede ser que debamos tenerlo en cuenta en algún momento del análisis multivariante si algún procedimiento no nos da resultados coherentes o esperados, pues puede deberse a la falta de normalidad de ciertas variables.

## Homocedasticidad

Para comparar la homocedasticidad, debemos distinguir por grupos dentro de unas mismas variables. El problema es que no tenemos más variables categóricas que el país. Sin embargo, la propia intuición y los gráficos de las variables numéricas nos sugieren ciertas distinciones entre los países por continentes. Por ello, introduciremos en el conjunto de datos una nueva variable 'continente', asociando cada país al continente en el que se sitúa la mayor parte de su superficie (La URSS ocupaba tres continentes distintos, pero la mayor parte de su superficie estaba en Asia) y estudiaremos la homocedasticidad entre los paises de Europa, Asia y África, ya que de América y de Oceanía hay muy pocos países en el *dataset*, por lo que no podríamos sacar conclusiones.

```{r,echo=F}
datos_aux <- datos[,-1]

datos_enteros$continente <- c(
  "africa", "africa", "america", "oceania",
  "america", "america", "america", "asia",
  "asia", "africa", "europa", "asia", 
  "europa", "europa", "asia", "asia",
  "asia", "asia", "europa", "asia", "asia",
  "africa", "america", "africa", "asia", "europa",
  "europa", "europa", "europa", "europa",
  "europa", "asia", "america", "asia")

```

```{r,echo=F}
ind<-which(datos_enteros$continente=="europa"|datos_enteros$continente=="asia"|datos_enteros$continente=="africa")
factores<-datos_enteros$continente[ind]
#Como se han eliminado los valores outlier, usamos con centro la media en vez de la mediana
#H0:homocedasticidad
apply(datos_aux[ind,], 2, function(x){
  if(leveneTest(x,as.factor(factores),center=median)$"Pr(>F)"[1]>0.05){
    "Existe homocedasticidad entre los grupos"
  }
  else{"No existe homocedasticidad entre los grupos"}
  })
```

Vemos que efectivamente en la mayoría de las variables, seleccionando los países según continente, se tiene la misma varianza. No se cumple para las variables `ZESPVIDA` y `ZPOBURB`, lo cual puede deberse a las grandes diferencias que existen sobre la esperanza de vida y las grandes ciudades en África. En los países africanos la esperanza de vida es menor en general y también hay menos grandes ciudades que en Asia y Europa, por lo que es de esperar que la distribución sea distinta.

## Exploración Descriptiva

Veremos ahora los principales estadísticos descriptivos comparando los datos originales con los tratados.

**Datos originales:**

```{r echo=F}
summary(datos_enteros[,c(-1,-13)])
```

**Datos tratados:**

```{r echo=F}
summary(datos[,-1])
```

Si nos fijamos, las únicas variables que han cambiado sus estadísticos son `ZTLIBROP`, a la cual se le imputó su único valor perdido, junto a `ZTEJERCI` y `ZTENERGI`, afectadas por la corrección de outliers.

\newpage

# Análisis exploratorio multivariante

A continuación aplicaremos una serie de técnicas de análisis multivariante, comenzando por comprobar precondiciones para después aplicar técnicas de reducción de la dimensión como el Análisis de Componentes Principales (al cual nos referiremos en varias ocasiones como ACP) o el Análisis Factorial (referido como AF).

```{r echo=F}
datos_pca = datos[,-1] # Eliminamos la primera columna, que es categórica
```

## Prerrequisitos

### Correlación

Antes de proceder con los análisis, debemos comprobar si existe correlación entre las variables, para ello recurrimos a la matriz de correlaciones, acompañada por un heatmap que nos ayudará a identificar mejor las posibles correlaciones más fuertes:

```{r echo=FALSE}
cor(datos_pca)
poly_cor<-hetcor(datos_pca)$correlations
ggcorrplot(poly_cor, type="lower",hc.order=T)
```

Observando la matriz, vemos que hay varias correlaciones notables. Por ejemplo, entre `ZESPVIDA` y `ZTMINFAN`, entre `ZPAGRICU` y `ZPOBURB` o entre `ZPSERVI` y `ZPAGRICU` existen fuertes correlaciones negativas y entre `ZPOBURB` y `ZPSERVI` una fuerte correlación positiva.

```{r}
cor(datos_pca$ZESPVIDA,datos_pca$ZTMINFAN)
cor(datos_pca$ZPAGRICU,datos_pca$ZPOBURB)
cor(datos_pca$ZPSERVI, datos_pca$ZPAGRICU)
```

Para comprobar estas y otras correlaciones usamos el contraste de esfericidad de Bartlett, el cual permite comprobar si las correlaciones son distintas de 0 de modo significativo. La hipótesis nula es que $\det(R)=1$ (siendo $R$ la matriz de correlaciones). Utilizaremos la función `cortest.bartlett` del paquete `psych`, que requiere de datos normalizados, pero estos datos ya sabemos que están normalizados, por lo que no es necesario ningún procesamiento adicional.

```{r}
cortest.bartlett(cor(datos_pca), n=100)
```

Como vemos, hemos obtenido un p-valor prácticamente nulo, e inferior a cualquier nivel de significación, por lo que rechazamos la hipótesis nula, es decir, asumimos que las variables no están incorreladas.

### Normalización y outliers

Por otra parte, recordamos que el análisis de componentes principales es muy sensible a *outliers*, pero ya nos ocupamos de ese asunto en [apartados anteriores](#valores-extremos), por lo que tenemos cumplidos todos los requisitos: datos correlados, sin valores extremos y normalizados.

## Análisis de componentes principales {#ACP}

Buscamos ahora realizar un estudio de la posibilidad de reducción de la dimensión mediante variables observables. Es decir, realizar un análisis de componentes principales. Veamos la matriz de pesos que se calcula:

```{r, echo=FALSE}
# scale y center están a True porque consideramos los datos normalizados
PCA<-prcomp(datos_pca, scale=T, center = T)
PCA$rotation 
```

La matriz obtenida como salida es es una matriz cuyas columnas son los coeficientes de las componentes principales, es decir, el peso de cada variable en la correspondiente componente principal. Observemos ahora alguna información relevante, como es la desviación típica de cada componente principal, así como el porcentaje de varianza explicado por cada componente principal.

```{r, echo=F}
summary(PCA)
```

Veamos ahora un análisis gráfico de la varianza explicada.

```{r echo=F}
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:11),
       aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Componente principal", y= " Proporción de varianza explicada")

```

El siguiente gráfico nos muestra la varianza explicada acumulada.

```{r, echo=FALSE}
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:11),
        aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
  geom_col(width = 0.5) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Proporción varianza acumulada")

```

Si observamos los gráficos, dependiendo del porcentaje de varianza explicada que fijemos, serían suficientes más o menos componentes principales, siendo observable que sólo con las tres primeras tendríamos más de un 75% de la varianza explicada. Sabiendo esto, es momento de elegir el número de componentes óptimo.

Utilizaremos el la regla de Abdi et al. (2010). Se promedia las varianzas explicadas por la componentes principales y se seleccionan aquellas cuya proporción de varianza explicada supera la media.

**Varianzas de las componentes principales:**

```{r, echo=FALSE}
PCA$sdev^2
```

**Media de las varianzas de las componentes principales:**

```{r, echo=FALSE}
mean(PCA$sdev^2)
```

Por tanto seleccionamos las **tres primeras componentes principales**.

Por último, haremos una visualización comparativa de estas tres componentes principales, viendo así qué variables tienen más peso en la definición de cada componente principal:

```{r, echo=FALSE}
# Entre la PC1 y la PC2
fviz_pca_var(PCA, axes=c(1,2),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

# Entre la PC1 y la PC3
fviz_pca_var(PCA,axes=c(1,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

#Entre la PC2 y la PC3
fviz_pca_var(PCA,axes=c(2,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

```

Como vemos, en la primera componente principal (gráficos 1 y 2) la mayoría de las variables tienen peso (flechas horizontales), aunque destacamos a `ZPOBDENS` y a `ZTEJERCI` que no parecen tener tanto peso, las flechas son más verticales y cortas. Mientras que si observamos la tercera componente principal (gráficos 2 y 3), vemos que `ZTEJERCI` tiene más peso en la tercera componente principal, mientras que `ZPOBDENS` tiende a repartirlo entre las tres componentes principales, sin llegar a tener demasiado en ninguna de las 3, pues su mayor peso se encontraba en la cuarta componente principal. Estas conclusiones estaban ya reflejadas en la matriz de rotación expuesta anteriormente, pero en estos gráficos pueden comprobarse más visualmente

## Análisis factorial

En la sección anterior estudiamos la matriz de correlaciones y comprobamos mediante el test de esfericidad de Bartlett que los datos están correlados. Sin embargo, utilizaremos una representación alternativa de la matriz de correlaciones para observar posibles agrupaciones en variables latentes:

```{r, echo=FALSE}
datos_fa = datos_pca
corrplot(cor(datos_fa), order = "hclust", tl.col='black', tl.cex=1,
         col=colorRampPalette(c("blue","white","red"))(200))
```

Antes de proceder con el análisis factorial es importante decidir el número óptimo de factores. Para ello usaremos el criterio que nos da el Scree plot o método del codo, en el cual el número óptimo de factores se corresponde con el número de puntos antes del codo:

```{r, echo=F}
scree(poly_cor) 
fa.parallel(poly_cor,n.obs=200,fa="fa",fm="minres")
```

Lo cierto es que los gráficos obtenidos no son muy concluyentes, pues no se puede observar claramente el 'codo', pero se puede estimar que el número de factores puede estar entre dos y tres. Para poder cerciorarnos utilizaremos el contraste de hipótesis que nos da la función `factanal`, testeando la hipótesis de que 2 factores son suficientes, y la hipótesis de que 3 factores son suficientes:

```{r, echo=FALSE}
factanal(datos_fa,factors=2, rotation="none")
factanal(datos_fa,factors=3, rotation="none")
```

A juzgar por el contraste, en el que se obtiene un p-valor muy cercano a 0 para `factors=2` y mayor para `factors=3`, concluiremos que el número óptimo de factores es 3.

> NOTA: En el gráfico correspondiente a componentes principales podemos revalidar la conclusión de que el número óptimo de componentes principales es 3 mediante un criterio distinto.

Ahora estimamos el modelo con 3 factores, al cual no aplicaremos ninguna rotación, ya que el modelo obtenido, en conclusiones, no difiere del que se obtendría con rotación alguna.

```{r}
modelo_varimax<-fa(poly_cor,nfactors = 3,rotate = "none",
                   fa="mle")
```

Veamos la matriz de pesos factorial, y para poder interpretarla de forma más visual utilizamos un diagrama.

```{r, echo=FALSE}
print(modelo_varimax$loadings,cut=0)
fa.diagram(modelo_varimax)
```

Nos podemos fijar en la matriz o en el diagrama que las variables `ZPAGRICU`, `ZPOBURB`, `ZPSERVI`, `ZESPVIDA`, `ZTMINFAN`, `ZTLIBROP`, `ZTMEDICO` y `ZTENERGI` forman un factor latente, el cual si nos fijamos en la matriz de correlación anteriormente presentada podemos observar muy presente. Por otra parte, las variables `ZTPOBACT` y `ZTEJERCI` forman por sí solas un factor latente. Esto ocurre porque estas variables no tienen nada que ver con las demás y por eso no se agrupan con otras. De hecho, si nos fijamos en la matriz de correlaciones, estas no tienen correlaciones fuertes con ninguna otra variable.

Estos hechos tan poco esperables en comparación con otros ejemplos se pueden deber, en parte, a que probablemente estos datos no están preparados para un análisis factorial y no tienen variables latentes subyacentes relevantes.

## Suposición de normalidad

A continuación se comprobará la normalidad univariante de cada variable y la multivariante del conjunto completo.

### Normalidad univariante

Recordemos que se usaron gráficos qqplot para examinar la normalidad univariante de cada variable en el apartado de [Normalidad](#normalidad), pero ahora además usaremos el *test de normalidad de Shapiro-Wilk* para comprobar las conclusiones.

```{r, echo=FALSE}
datos_da = datos_fa
datos_tidy <- melt(datos, value.name = "valor")
aggregate(formula = valor ~ variable, data = datos_tidy,
          FUN = function(x){shapiro.test(x)$p.value})
```

Como vemos, hay p-valores muy pequeños en varias variables. Destacamos a `ZTMINFAN` y a `ZTLIBROP`, que tienen p-valores especialmente pequeños. Las demás, a un nivel de significación del 1% podemos afirmar que tienen distribución normal. Recordemos además que los gráficos `qqplot` nos dejaban intuir que `ZPOBDENS` y `ZMINFAN` y tampoco seguían una distribución normal univariante. La falta de normalidad univariante es mal presagio de cara a la comprobación de normalidad multivariante, pues según la teoría de la DNM implica que no se puede dar el supuesto de normalidad multivariante, por lo que habrá que tenerla en cuenta si en próximos análisis no se obtienen las conclusiones deseadas.


### Normalidad multivariante

Para comprobar la normalidad multivariante utilizaremos algunos test como el *test de Royston* y el *test de Henze-Zirkler*. El resultado de estos test puede verse afectado por la presencia de outliers. Aclaramos que en este caso nos referimos a outliers dentro de la hipotética distribución normal multivariante, los outliers anteriormente tratados son univariantes, propios a la distribución de cada variable por separado y no como la distribución de un vector aleatorio.

En el siguiente gráfico podemos comprobar que efectivamente la distribución cuenta con 11 observaciones que se consideran outliers, lo cual, como hemos comentado anteriormente, puede influir en los resultados de los tests de hipótesis.

```{r, echo=FALSE}
outliers <- mvn(data = datos_da, mvnTest = "hz", multivariateOutlierMethod = "quan")
```

Sin más dilación procedemos a ejecutar los test de normalidad multivariante, en concreto, los test de *Royston* y de *Henze-Zirkler*.

```{r, echo=FALSE}
royston_test <- mvn(data = datos_da, mvnTest = "royston", multivariatePlot = "qq")

royston_test$multivariateNormality

hz_test <- mvn(data = datos_da, mvnTest = "hz")
hz_test$multivariateNormality

```

Como podemos comprobar, el test de Royston nos ofrece un p-valor casi nulo, por lo que no podemos concluir a partir de este test que los datos siguen una DNM. Sin embargo, el test de HZ nos devuelve un p-valor que a un nivel de significación del 1% (incluso del 5%) podemos concluir que los datos siguen una DNM. Esta discrepancia entre tests puede deberse a los outliers comentados anteriormente, a la falta de normalidad univariante, al pequeño tamaño de la muestra o a cualquier asunción realizada en apartados anteriores.

De hecho, el test de Shapiro-Wilk tiende a rechazar la hipótesis nula si el tamaño de la muestra es inferior a 50, y en este caso es 34, por lo que es normal encontrarnos variables que no siguen DNM. Sin embargo, podríamos fiarnos del test multivariante de H-Z para asumir que la distribución es una DNM, y por tanto, todas las variables que componen el vector aleatorio tienen que seguir distribución normal univariante.

## Aplicación de técnicas de ciencia de datos

Contamos con un conjunto de datos que no es susceptible de aplicar clasificación, pues no tenemos una variable a predecir ni ejemplos para entrenar un clasificador. Sin embargo, podemos tomarnos este como un problema de aprendizaje no supervisado, en el que no tenemos asignada etiqueta alguna a los ejemplos, pero nos gustaría agruparlos de acuerdo a los valores de las variables que tenemos. Para ello, aplicaremos alguna técnica de *clustering*.

Como ya tenemos variables numéricas normalizadas y los métodos por defecto utilizan la distancia euclídea, no tenemos que especificar ninguna distancia distinta. La siguiente matriz de distancias muestra en rojo aquellos países que presentan grandes disimilirades (distancias), frente a aquellos que parecen más cercanos en azul.

```{r, echo=FALSE}
datos_ca <- datos_fa
rownames(datos_ca) <- datos$PAIS
distance<- get_dist(datos_ca)
fviz_dist(distance, gradient = list(low ="#00AFBB", mid = "white", high = "#FC4E07"))
```

De un primer vistazo vemos cierta agrupación en términos de cercanías.

### Aplicación del algoritmo de *clustering* 'K-Medias'

El principal defecto que tiene el algoritmo 'K-Medias' es que hay que fijar de antemano el número de clusters que se van a calcular. Sin embargo, hay métodos que permiten fijar de antemano el número óptimo de clusters. En este caso utilizaremos el método de la silueta:

```{r, echo=FALSE}
set.seed(123)

fviz_nbclust(datos_ca, kmeans, method = "silhouette")
```

El gráfico nos dice que el número óptimo de clusters es 2, pues es el que nos proporciona un mayor coeficiente *silhouette*. Recordemos que el coeficiente *silhouette* es una medida de la calidad de los clusters, resumiendo cómo de adecuado es un objeto dentro de su cluster. Por tanto, haremos un clustering con dos clusters utilizando *K-Means*:

```{r, echo=FALSE}
k2 <- kmeans(datos_ca, centers = 2, nstart = 25)
fviz_cluster(k2,data=datos_ca)
```

En el gráfico podemos ver un resumen del resultado del clustering. Destacar que los dos clusters no se solapan, característica muy deseable en este ámbito. Por otra parte, si nos fijamos en las dimensiones del gráfico, están acompañadas de un porcentaje: 56% en el eje horizontal y 14.9% en el eje vertical. No es casualidad que coincidan con el 55.99% de varianza explicada por la primera componente principal y el 14.91% de varianza explicada por la segunda componente principal, como puede comprobarse en el apartado de [Análisis de componentes principales](#ACP). Es decir, en el gráfico podemos ver representado el agrupamiento bajo un 70.90% de la varianza explicada.

### Análisis de resultados

La salida ofrecida por el algoritmo es la siguiente:

```{r, echo=FALSE}
print(k2)
```

Llama la atención que en la mayoría de las variables un cluster tiene media positiva y el otro negativa, como si en un cluster se encontraran países con dicha medida alta y en el otro baja, y este patrón se repite en la mayoría de las variables.
